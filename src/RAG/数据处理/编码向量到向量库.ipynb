{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/src/RAG')\n",
    "from vector_store_utils import *\n",
    "import sys\n",
    "sys.path.append('/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/src')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'qvrr9a7mfp059p452bfs',\n",
       " 'title': '# 独立权 管辖权 平等权 自卫权',\n",
       " 'content': '\\n◆ 独立权，是指主权国家拥有按照自己的意志处理内政、外交事务而不受他国控制和干涉的权利。  \\n\\n◆ 管辖权，是指主权国家对其领域内的一切人和物具有管辖的权利。  \\n\\n◆ 平等权，是指主权国家不论大小、强弱，也不论政治、经济、意识形态和社会制度有何差异，在国际法上的地位一律平等。  \\n\\n◆ 自卫权，是指主权国家拥有保卫自己的生存和独立的权利。  \\n',\n",
       " 'grade': '选择性必修1',\n",
       " 'subject': '高中政治',\n",
       " 'modify_content': '独立权，是指主权国家拥有按照自己的意志处理内政、外交事务而不受他国控制和干涉的权利。\\n\\n管辖权，是指主权国家对其领域内的一切人和物具有管辖的权利。\\n\\n平等权，是指主权国家不论大小、强弱，也不论政治、经济、意识形态和社会制度有何差异，在国际法上的地位一律平等。\\n\\n自卫权，是指主权国家拥有保卫自己的生存和独立的权利。',\n",
       " '课本内容': '独立权，是指主权国家拥有按照自己的意志处理内政、外交事务而不受他国控制和干涉的权利。\\n\\n管辖权，是指主权国家对其领域内的一切人和物具有管辖的权利。\\n\\n平等权，是指主权国家不论大小、强弱，也不论政治、经济、意识形态和社会制度有何差异，在国际法上的地位一律平等。\\n\\n自卫权，是指主权国家拥有保卫自己的生存和独立的权利。',\n",
       " '课外内容': '在国际法框架下，主权国家四大基本权利的现实实践具有多维度的体现。以独立权为例，2023年不结盟运动峰会上，印度尼西亚公开拒绝大国干预其镍矿出口政策，援引《联合国宪章》第二条第七款阐明其经济自主权。管辖权在孟晚舟引渡案中得到具象化呈现：加拿大依据属地管辖原则实施司法程序，而中国则启动领事保护机制主张属人管辖权，该案件成为研究国家管辖权竞合冲突的经典案例。平等权的当代实践体现于2022年联合国大会表决俄国决议草案时，圣多美和普林西比（人口21万）与印度（13亿人口）享有同等表决权重，这一制度设计深刻反映了主权平等原则。自卫权的法律边界在2020年伊朗圣城旅指挥官苏莱曼尼遇袭事件中引发全球讨论，多国法学家援引《联合国宪章》第51条，就预防性自卫的合法性展开激烈辩论。日内瓦国际关系研究院2023年报告指出，四大权利在实践中存在重叠与张力，如美国依据《云法案》主张数据跨境管辖权时，常与他国独立权产生冲突，这种现象推动着国际法体系的动态演进。'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alll = getData(\"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/高中/md/合并/2.加课外内容.json\")\n",
    "alll[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4107"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"从全部数据导入\"\n",
    "import os\n",
    "if data_type == \"从未清洗数据得到\":\n",
    "    subjects = [\"高中生物\",\"高中历史\",\"高中地理\",\"高中政治\"]\n",
    "    all_res  = []\n",
    "    all_text = []\n",
    "    for sub in subjects:\n",
    "        paths = get_absolute_file_paths(os.path.join(\"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/初中\",sub),\"json\")\n",
    "        for path in paths:\n",
    "            grade = path.split(\"/\")[-1].split(\" \")[-1].split(\".\")[0]\n",
    "            res = getData(path)\n",
    "            new_res = []\n",
    "            new_text = []\n",
    "            for item in res:\n",
    "                new_res.append({\"subject\":sub,\"grade\":grade,\"title\":item['title'].strip(),\"content\":item['content'].strip()})\n",
    "                new_text.append(item['content'].strip())\n",
    "            all_res.extend(new_res)\n",
    "            all_text.extend(new_text)\n",
    "else:\n",
    "    alll = getData(\"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/高中/md/合并/2.加课外内容.json\")\n",
    "    all_res  = []\n",
    "    all_text = []\n",
    "    for item in alll:\n",
    "        all_res.append({\"subject\":item['subject'].strip(),\"grade\":item['grade'].strip(),\"title\":item['title'].strip(),\"content\":item['课本内容'].strip()})\n",
    "        all_text.append(item['content'].strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 1.06 MiB is free. Process 2879711 has 28.74 GiB memory in use. Process 3019153 has 48.01 GiB memory in use. Process 3134648 has 2.54 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 16.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m ef = BGEM3EmbeddingFunction(\n\u001b[32m      4\u001b[39m     model_name = \u001b[33m\"\u001b[39m\u001b[33m/hpc2hdd/home/fye374/models/BAAI/bge-m3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     use_fp16=\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[32m      6\u001b[39m     device=\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m dense_dim = ef.dim[\u001b[33m\"\u001b[39m\u001b[33mdense\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m docs_embeddings = \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/pymilvus/model/hybrid/bge_m3.py:79\u001b[39m, in \u001b[36mBGEM3EmbeddingFunction.__call__\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) -> Dict:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/pymilvus/model/hybrid/bge_m3.py:90\u001b[39m, in \u001b[36mBGEM3EmbeddingFunction._encode\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) -> Dict:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     results = {}\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._encode_config[\u001b[33m\"\u001b[39m\u001b[33mreturn_dense\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:295\u001b[39m, in \u001b[36mM3Embedder.encode\u001b[39m\u001b[34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: return_sparse = \u001b[38;5;28mself\u001b[39m.return_sparse\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_colbert_vecs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: return_colbert_vecs = \u001b[38;5;28mself\u001b[39m.return_colbert_vecs\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dense\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_colbert_vecs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_colbert_vecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/FlagEmbedding/abc/inference/AbsEmbedder.py:264\u001b[39m, in \u001b[36mAbsEmbedder.encode\u001b[39m\u001b[34m(self, sentences, batch_size, max_length, convert_to_numpy, instruction, instruction_format, **kwargs)\u001b[39m\n\u001b[32m    260\u001b[39m         sentences = [\u001b[38;5;28mself\u001b[39m.get_detailed_instruct(instruction_format, instruction, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[32m    261\u001b[39m                      sentences]\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentences, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.target_devices) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_single_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m.pool = \u001b[38;5;28mself\u001b[39m.start_multi_process_pool(AbsEmbedder._encode_multi_process_worker)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:340\u001b[39m, in \u001b[36mM3Embedder.encode_single_device\u001b[39m\u001b[34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, device, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.use_fp16 = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_fp16: \u001b[38;5;28mself\u001b[39m.model.half()\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m    343\u001b[39m input_was_string = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/langmanus/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 79.33 GiB of which 1.06 MiB is free. Process 2879711 has 28.74 GiB memory in use. Process 3019153 has 48.01 GiB memory in use. Process 3134648 has 2.54 GiB memory in use. Of the allocated memory 2.12 GiB is allocated by PyTorch, and 16.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "ef = BGEM3EmbeddingFunction(\n",
    "    model_name = \"/hpc2hdd/home/fye374/models/BAAI/bge-m3\",\n",
    "    use_fp16=False, \n",
    "    device=\"cuda:0\")\n",
    "\n",
    "\n",
    "dense_dim = ef.dim[\"dense\"]\n",
    "docs_embeddings = ef(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection subjects not found, 请创建collection\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus\")\n",
    "from src.config.rag import DB_URI,COLLECTION_NAME\n",
    "from src.RAG.vector_store_utils import *\n",
    "db_uri = DB_URI\n",
    "col_name = COLLECTION_NAME\n",
    "col = get_collection(db_uri,col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection_info = col.describe()\n",
    "# # collection_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = col.query_iterator(\n",
    "#     batch_size=10,\n",
    "#     expr=None,\n",
    "#     output_fields=[\"subject\"]\n",
    "# )\n",
    "\n",
    "# value_set = set()\n",
    "\n",
    "# while True:\n",
    "#     result = iterator.next()\n",
    "#     if not result:\n",
    "#         iterator.close()\n",
    "#         break\n",
    "    \n",
    "#     for item in result:\n",
    "#         value_set.add(item[\"subject\"])\n",
    "# col.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection subjects 创建成功，并返回它\n"
     ]
    }
   ],
   "source": [
    "col = create_collection(db_uri,col_name, dense_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4107/4107 [00:48<00:00, 84.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加成功\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "add_data(col, all_res,docs_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检索示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milvus_model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "ef = BGEM3EmbeddingFunction(\n",
    "    model_name = \"/hpc2hdd/home/fye374/models/BAAI/bge-m3\",\n",
    "    use_fp16=False, \n",
    "    device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = input(\"Enter your search query: \")\n",
    "# print(query)\n",
    "query = \"生物微分子\"\n",
    "query_embeddings = ef([query])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/src/RAG')\n",
    "from retrieval import *\n",
    "from reranker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_uri = \"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/src/RAG/vector_store/subjects.db\"\n",
    "col_name = \"subjects\"\n",
    "col = get_collection(db_uri,col_name)\n",
    "dense_results = dense_search(col, query_embeddings[\"dense\"][0])\n",
    "sparse_results = sparse_search(col, query_embeddings[\"sparse\"]._getrow(0))\n",
    "hybrid_results = hybrid_search(\n",
    "    col,\n",
    "    query_embeddings[\"dense\"][0],\n",
    "    query_embeddings[\"sparse\"]._getrow(0),\n",
    "    sparse_weight=0.7,\n",
    "    dense_weight=1.0,\n",
    ")\n",
    "# /hpc2hdd/home/fye374/ZWZ_Other/quizmanus/src/RAG/vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank(query, hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('langmanus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6826872b2e90c4a5b89db49ae6de9319427673b2cab0f057a3d6baa327072350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
