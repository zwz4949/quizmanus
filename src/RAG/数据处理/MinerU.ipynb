{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def get_absolute_file_paths(absolute_dir,file_type)->List[str]:\n",
    "    '''\n",
    "    absolute_dir: 文件夹\n",
    "    file_type: \"md\",\"json\"...\n",
    "    '''\n",
    "    json_files = [os.path.join(absolute_dir,f) for f in os.listdir(absolute_dir) if f.endswith(f\".{file_type}\")]\n",
    "    return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\n",
    "from magic_pdf.data.dataset import PymuDocDataset\n",
    "from magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\n",
    "from magic_pdf.config.enums import SupportedPdfParseMethod\n",
    "\n",
    "def MinerU(pdf_file_name):\n",
    "    # args\n",
    "    # pdf_file_name = \"small_ocr.pdf\"  # replace with the real pdf path\n",
    "    name_without_suff = pdf_file_name.split(\".\")[0]\n",
    "\n",
    "    # prepare env\n",
    "    local_image_dir, local_md_dir = \"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/高中/md/images\", \"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/高中/md\"\n",
    "    image_dir = str(os.path.basename(local_image_dir))\n",
    "\n",
    "    os.makedirs(local_image_dir, exist_ok=True)\n",
    "\n",
    "    image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(\n",
    "        local_md_dir\n",
    "    )\n",
    "\n",
    "    # read bytes\n",
    "    reader1 = FileBasedDataReader(\"\")\n",
    "    pdf_bytes = reader1.read(pdf_file_name)  # read the pdf content\n",
    "\n",
    "    # proc\n",
    "    ## Create Dataset Instance\n",
    "    ds = PymuDocDataset(pdf_bytes)\n",
    "\n",
    "    ## inference\n",
    "    if ds.classify() == SupportedPdfParseMethod.OCR:\n",
    "        infer_result = ds.apply(doc_analyze, ocr=True)\n",
    "\n",
    "        ## pipeline\n",
    "        pipe_result = infer_result.pipe_ocr_mode(image_writer)\n",
    "\n",
    "    else:\n",
    "        infer_result = ds.apply(doc_analyze, ocr=False)\n",
    "\n",
    "        ## pipeline\n",
    "        pipe_result = infer_result.pipe_txt_mode(image_writer)\n",
    "\n",
    "    # ### draw model result on each page\n",
    "    # infer_result.draw_model(os.path.join(local_md_dir, f\"{name_without_suff}_model.pdf\"))\n",
    "\n",
    "    # ### get model inference result\n",
    "    # model_inference_result = infer_result.get_infer_res()\n",
    "\n",
    "    # ### draw layout result on each page\n",
    "    # pipe_result.draw_layout(os.path.join(local_md_dir, f\"{name_without_suff}_layout.pdf\"))\n",
    "\n",
    "    # ### draw spans result on each page\n",
    "    # pipe_result.draw_span(os.path.join(local_md_dir, f\"{name_without_suff}_spans.pdf\"))\n",
    "\n",
    "    # ### get markdown content\n",
    "    # md_content = pipe_result.get_markdown(image_dir)\n",
    "\n",
    "    ### dump markdown\n",
    "    pipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", image_dir)\n",
    "\n",
    "    # ### get content list content\n",
    "    # content_list_content = pipe_result.get_content_list(image_dir)\n",
    "\n",
    "    # ### dump content list\n",
    "    # pipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", image_dir)\n",
    "\n",
    "    # ### get middle json\n",
    "    # middle_json_content = pipe_result.get_middle_json()\n",
    "\n",
    "    # ### dump middle json\n",
    "    # pipe_result.dump_middle_json(md_writer, f'{name_without_suff}_middle.json')\n",
    "    print(f\"完成{pdf_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_absolute_file_paths(\"/hpc2hdd/home/fye374/ZWZ_Other/quizmanus/dataset/课本md/高中/ori/高中地理\",\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-18 14:07:09.812\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.data.dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mlang: None\u001b[0m\n",
      "\u001b[32m2025-04-18 14:07:14.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.libs.pdf_check\u001b[0m:\u001b[36mdetect_invalid_chars\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mcid_count: 264, text_len: 6692, cid_chars_radio: 0.056896551724137934\u001b[0m\n",
      "\u001b[32m2025-04-18 14:07:14.521\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmagic_pdf.filter.pdf_classify_by_type\u001b[0m:\u001b[36mclassify\u001b[0m:\u001b[36m335\u001b[0m - \u001b[33m\u001b[1mOCR needed based on classification result, by_image_area: True, by_text: True, by_avg_words: True, by_img_num: True, by_img_narrow_strips: True, by_invalid_chars: False\u001b[0m\n",
      "\u001b[32m2025-04-18 14:07:25.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmagic_pdf.model.doc_analyze_by_custom_model\u001b[0m:\u001b[36mmay_batch_image_analyze\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mgpu_memory: 79 GB, batch_ratio: 16\u001b[0m\n",
      "Layout Predict: 100%|██████████| 134/134 [00:05<00:00, 26.41it/s]\n",
      "OCR-det Predict: 100%|██████████| 134/134 [00:22<00:00,  5.92it/s]\n",
      "Table Predict:   6%|▋         | 1/16 [00:01<00:29,  1.94s/it]"
     ]
    }
   ],
   "source": [
    "for path in paths:  \n",
    "    MinerU(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 ('langmanus': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6826872b2e90c4a5b89db49ae6de9319427673b2cab0f057a3d6baa327072350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
